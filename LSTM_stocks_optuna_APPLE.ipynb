{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://github.com/wcneill/jn-ml-textbook/blob/master/Deep%20Learning/04%20Recurrent%20Networks/pytorch13b_LSTM.ipynb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ta import add_all_ta_features #pip install --upgrade ta https://github.com/bukosabino/ta https://medium.datadriveninvestor.com/predicting-the-stock-market-with-python-bba3cf4c56ef\n",
    "from fastai.tabular.all import add_datepart #pip install fastai https://docs.fast.ai/tabular.core.html https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [12,8]\n",
    "viz_dict = {\n",
    "    'axes.titlesize':18,\n",
    "    'axes.labelsize':16,\n",
    "}\n",
    "sns.set_context(\"notebook\", rc=viz_dict)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(date_string):\n",
    "    year, month, day = [int(i) for i in date_string.split('-')]\n",
    "    return datetime(year=year, month=month, day=day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df):\n",
    "    '''\n",
    "    Converts the dataframe to the appropriate types.\n",
    "    '''\n",
    "    # Convert data to float32 for PyTorch\n",
    "    df = df.astype(np.float32)\n",
    "\n",
    "    # I'm going to convert this to train and test instead of train and valid.\n",
    "    # Train will include validation set through cross validation.\n",
    "    y = np.where(df['Close'].shift(-1) > df['Close'], 1, -1)\n",
    "    df = df.drop(['Close'], axis=1)\n",
    "    if 'trend_psar_down' in df.columns:\n",
    "        df = df.drop(['trend_psar_down', 'trend_psar_up', 'Adj Close'], axis=1)\n",
    "    # Split training and validation data\n",
    "    split = int(0.8 * len(df))\n",
    "    x_train = df[:split].to_numpy() #.reshape(-1, 1)\n",
    "    x_test = df[split:].to_numpy() #.reshape(-1, 1)\n",
    "\n",
    "    y_train = y[:split].reshape(-1, 1)\n",
    "    y_test = y[split:].reshape(-1, 1)\n",
    "\n",
    "    # scale data: MOVED THIS TO HAPPEN WITHIN CROSS VALIDATION\n",
    "\n",
    "    # t_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # v_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # x_train = t_scaler.fit_transform(x_train)\n",
    "    # x_test = v_scaler.fit_transform(x_test)\n",
    "\n",
    "    # convert training data to tensor\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, window):\n",
    "    \"\"\"\n",
    "    Takes data with shape (n_samples, n_features) and creates mini-batches\n",
    "    with shape (1, window). \n",
    "    \"\"\"\n",
    "    x_data, y_data = data\n",
    "    L = len(x_data)\n",
    "    for i in range(L - window):\n",
    "        x_sequence = x_data[i:i + window]\n",
    "        y_sequence = y_data[i:i + window]\n",
    "        yield x_sequence, y_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stocksLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_p):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_p)\n",
    "        self.fc   = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hs):\n",
    "   \n",
    "        out, hs = self.lstm(x, hs)           # out.shape = (batch_size, seq_len, hidden_size)\n",
    "        out = out.view(-1, self.hidden_size) # out.shape = (seq_len, hidden_size)     \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_set, valid_data=None, lr=0.001, print_every=100):\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        hs = None\n",
    "        t_loss = 0\n",
    "        for x, y in get_batches(train_set, 12):\n",
    "\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Create batch_size dimension\n",
    "            x = x.unsqueeze(0)\n",
    "            out, hs = model(x, hs)\n",
    "            hs = tuple([h.data for h in hs])\n",
    "            \n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            t_loss += loss.item()\n",
    "            \n",
    "        if valid_data is not None:\n",
    "                model.eval()\n",
    "                val_x, val_y = valid_data\n",
    "                val_x = val_x.unsqueeze(0)\n",
    "                preds, _ = model(val_x, hs)\n",
    "                v_loss = criterion(preds, val_y)\n",
    "                valid_loss.append(v_loss.item())\n",
    "                \n",
    "                model.train()\n",
    "            \n",
    "        train_loss.append(np.mean(t_loss))\n",
    "            \n",
    "        if print_every:    \n",
    "            if e % print_every == 0:\n",
    "                print(f'Epoch {e}:\\nTraining Loss: {train_loss[-1]}')\n",
    "                if valid_data is not None:\n",
    "                    print(f'Validation Loss: {valid_loss[-1]}')\n",
    "        \n",
    "    return train_loss, valid_loss\n",
    "    \n",
    "#     plt.figure(figsize=[8., 6.])\n",
    "#     plt.plot(train_loss, label='Training Loss')\n",
    "#     plt.plot(valid_loss, label='Validation Loss')\n",
    "#     plt.title('Loss vs Epochs')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross_valid(model, epochs, lr, training_data):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    X_train, y_train = training_data\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    for train_index, validation_index in tscv.split(X_train):\n",
    "        # print(\"TRAIN:\", train_index, \"VALIDATION:\", validation_index)\n",
    "        # Split training and validation data\n",
    "        # split = int(0.8 * len(df))\n",
    "        # x_train = X_train[train_index].to_numpy() #.reshape(-1, 1)\n",
    "        # x_valid = X_train[validation_index].to_numpy() #.reshape(-1, 1)\n",
    "        x_train_fold = X_train[train_index]\n",
    "        x_valid_fold = X_train[validation_index]\n",
    "\n",
    "        y_train_fold = y_train[train_index].reshape(-1, 1)\n",
    "        y_valid_fold = y_train[validation_index].reshape(-1, 1)\n",
    "\n",
    "        # scale data\n",
    "        # t_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        # v_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        t_scaler = MinMaxScaler()\n",
    "        x_train_fold = t_scaler.fit_transform(x_train_fold)\n",
    "        x_valid_fold = t_scaler.transform(x_valid_fold)\n",
    "\n",
    "        # convert training data to tensor\n",
    "        x_train_fold = torch.tensor(x_train_fold, dtype=torch.float32)\n",
    "        x_valid_fold = torch.tensor(x_valid_fold, dtype=torch.float32)\n",
    "\n",
    "        # y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32)\n",
    "        # y_valid_fold = torch.tensor(y_valid_fold, dtype=torch.float32)\n",
    "\n",
    "        # Create training and validation set:\n",
    "        train_data_fold = (x_train_fold, y_train_fold)\n",
    "        valid_data_fold = (x_valid_fold, y_valid_fold)\n",
    "        \n",
    "        train_loss, valid_loss = train(model, epochs, train_data_fold, valid_data=valid_data_fold, lr=lr, print_every=None)\n",
    "                #Only append losses from the last epoch\n",
    "        train_losses.append(train_loss[-1])\n",
    "        valid_losses.append(valid_loss[-1])\n",
    "\n",
    "    # Compute average loss of folds             \n",
    "    train_losses_avg = sum(train_losses) / len(train_losses)   \n",
    "    valid_losses_avg = sum(valid_losses) / len(valid_losses)  \n",
    "    return train_losses_avg, valid_losses_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "              'hidden_size': trial.suggest_int('hidden_size', 40, 100, 20),\n",
    "              'num_layers': trial.suggest_int('num_layers', 2, 4),\n",
    "              'dropout_p': trial.suggest_float('dropout_p', 0.0, 0.2, step=0.1)\n",
    "              }\n",
    "    \n",
    "    input_size = x_train.size()[1]  # 21 in current implementation\n",
    "    hidden_size = 100\n",
    "    #num_layers = 1     # tuned by optuna\n",
    "    output_size = 1\n",
    "\n",
    "    model = stocksLSTM(input_size, params['hidden_size'], params['num_layers'], output_size, params['dropout_p'])\n",
    "    \n",
    "    train_losses_avg, valid_losses_avg = train_cross_valid(model, EPOCHS, params['learning_rate'], train_data)\n",
    "\n",
    "    return valid_losses_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(df):\n",
    "    '''\n",
    "    Find the optimal hyperparameters given the training data.\n",
    "    '''\n",
    "    x_train, y_train, x_test, y_test = convert_df(df)\n",
    "    train_data = (x_train, y_train)\n",
    "    test_data = (x_test, y_test)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(objective, n_trials=30)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    lr = best_trial.params.get('learning_rate')\n",
    "    hs = best_trial.params.get('hidden_size')\n",
    "    nl = best_trial.params.get('num_layers')\n",
    "    dp = best_trial.params.get('dropout_p')\n",
    "    return lr, hs, nl, dp, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the DataFrames\n",
    "\n",
    "In this cell, we obtain the four desired DataFrames corresponding to 1.) long-term with all features, 2.) short-term with all features, 3.) short-term with selected features (including StockTwits sentiment), and 4.) short-term with selected features (excluding StockTwits sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandrosaenz/opt/anaconda3/lib/python3.9/site-packages/ta/trend.py:769: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "/Users/alejandrosaenz/opt/anaconda3/lib/python3.9/site-packages/ta/trend.py:774: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('final/Stock_Stalkers/data/AAPL_data.csv')\n",
    "df = pd.read_csv('data/AAPL_data.csv')\n",
    "df.dropna()\n",
    "\n",
    "df[\"Date\"]=pd.to_datetime(df.Date, format=\"%Y-%m-%d\")\n",
    "df.index=df['Date']\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "add_datepart(df, 'Date', drop=False)\n",
    "df.drop('Elapsed', axis=1, inplace=True)\n",
    "\n",
    "df = add_all_ta_features(\n",
    "    df, high=\"High\", low=\"Low\", open=\"Open\", close=\"Close\", volume=\"Volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All parameters, whole time period:\n",
    "df_all = df\n",
    "df_all = df[df['Date'] >= '2017-08-09']\n",
    "df_all.index=df_all['Date']\n",
    "df_all = df_all.loc[:, df_all.columns !='Date']\n",
    "# All parameters, short time period:\n",
    "df_all_short = df[df['Date'] >= '2022-01-01']\n",
    "# Add sentiment\n",
    "df_all_short.index = np.array(range(len(df_all_short)))\n",
    "# df_sentiment = pd.read_csv('final/Stock_Stalkers/data/AAPL_byday_RoBERTa.csv')\n",
    "df_sentiment = pd.read_csv('data/AAPL_byday_RoBERTa.csv')\n",
    "# Drop the first empty column for AAPL\n",
    "df_sentiment = df_sentiment.iloc[: , 1:]\n",
    "df_sentiment.date = df_sentiment.date.apply(convert)\n",
    "df_sentiment.rename(columns={'date':'Date'}, inplace=True)\n",
    "df_all_short = df_all_short.merge(df_sentiment, how='inner', on='Date').fillna(0)\n",
    "df_all_short.index=df_all_short['Date']\n",
    "df_all_short = df_all_short.loc[:, df_all_short.columns !='Date']\n",
    "\n",
    "# Selected parameters, with sentiment:\n",
    "# sentiment_params = pd.read_csv('feature_selection_AAPL_Sentiment.csv').feature[:20]\n",
    "sentiment_params = pd.read_csv('data/feature_selection_AAPL_Sentiment.csv').feature[:20]\n",
    "df_selected_sentiment = df_all_short[list(sentiment_params) + ['Close']]\n",
    "\n",
    "# Selected parameters, no sentiment:\n",
    "# noSentiment_params = pd.read_csv('feature_selection_AAPL_NoSentiment.csv').feature[:20]\n",
    "noSentiment_params = pd.read_csv('data/feature_selection_AAPL_NoSentiment.csv').feature[:20]\n",
    "\n",
    "df_selected_noSentiment = df_all_short[list(noSentiment_params) + ['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>momentum_pvo_signal</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "      <th>bullish</th>\n",
       "      <th>bearish</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>182.880005</td>\n",
       "      <td>177.710007</td>\n",
       "      <td>177.830002</td>\n",
       "      <td>182.009995</td>\n",
       "      <td>104487900.0</td>\n",
       "      <td>181.778397</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143274</td>\n",
       "      <td>-8.251705</td>\n",
       "      <td>-2.942693</td>\n",
       "      <td>-5.309012</td>\n",
       "      <td>173.552778</td>\n",
       "      <td>2.500415</td>\n",
       "      <td>2.469666</td>\n",
       "      <td>406.815182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>182.940002</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>182.630005</td>\n",
       "      <td>179.699997</td>\n",
       "      <td>99310400.0</td>\n",
       "      <td>179.471344</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189433</td>\n",
       "      <td>-6.848703</td>\n",
       "      <td>-3.723895</td>\n",
       "      <td>-3.124807</td>\n",
       "      <td>174.349880</td>\n",
       "      <td>-1.269160</td>\n",
       "      <td>-1.277282</td>\n",
       "      <td>400.382888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>180.169998</td>\n",
       "      <td>174.639999</td>\n",
       "      <td>179.610001</td>\n",
       "      <td>174.919998</td>\n",
       "      <td>94537600.0</td>\n",
       "      <td>174.697418</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407488</td>\n",
       "      <td>-6.083336</td>\n",
       "      <td>-4.195783</td>\n",
       "      <td>-1.887553</td>\n",
       "      <td>174.357792</td>\n",
       "      <td>-2.659988</td>\n",
       "      <td>-2.696006</td>\n",
       "      <td>387.072762</td>\n",
       "      <td>545</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>175.300003</td>\n",
       "      <td>171.639999</td>\n",
       "      <td>172.699997</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>96904000.0</td>\n",
       "      <td>171.781143</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653812</td>\n",
       "      <td>-5.210295</td>\n",
       "      <td>-4.398686</td>\n",
       "      <td>-0.811609</td>\n",
       "      <td>174.294544</td>\n",
       "      <td>-1.669334</td>\n",
       "      <td>-1.683424</td>\n",
       "      <td>378.941893</td>\n",
       "      <td>548</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>174.139999</td>\n",
       "      <td>171.029999</td>\n",
       "      <td>172.889999</td>\n",
       "      <td>172.169998</td>\n",
       "      <td>86709100.0</td>\n",
       "      <td>171.950928</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791825</td>\n",
       "      <td>-5.341759</td>\n",
       "      <td>-4.587300</td>\n",
       "      <td>-0.754458</td>\n",
       "      <td>174.226442</td>\n",
       "      <td>0.098836</td>\n",
       "      <td>0.098787</td>\n",
       "      <td>379.415261</td>\n",
       "      <td>478</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-22</th>\n",
       "      <td>167.869995</td>\n",
       "      <td>161.500000</td>\n",
       "      <td>166.460007</td>\n",
       "      <td>161.789993</td>\n",
       "      <td>84775200.0</td>\n",
       "      <td>161.789993</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658369</td>\n",
       "      <td>-4.834595</td>\n",
       "      <td>-5.769592</td>\n",
       "      <td>0.934997</td>\n",
       "      <td>170.303486</td>\n",
       "      <td>-2.782120</td>\n",
       "      <td>-2.821555</td>\n",
       "      <td>350.511661</td>\n",
       "      <td>756</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-25</th>\n",
       "      <td>163.169998</td>\n",
       "      <td>158.460007</td>\n",
       "      <td>161.119995</td>\n",
       "      <td>162.880005</td>\n",
       "      <td>96046400.0</td>\n",
       "      <td>162.880005</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.666083</td>\n",
       "      <td>-2.708870</td>\n",
       "      <td>-5.157448</td>\n",
       "      <td>2.448578</td>\n",
       "      <td>169.837855</td>\n",
       "      <td>0.673720</td>\n",
       "      <td>0.671461</td>\n",
       "      <td>353.546848</td>\n",
       "      <td>951</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-26</th>\n",
       "      <td>162.339996</td>\n",
       "      <td>156.720001</td>\n",
       "      <td>162.250000</td>\n",
       "      <td>156.800003</td>\n",
       "      <td>95623200.0</td>\n",
       "      <td>156.800003</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870720</td>\n",
       "      <td>-1.087910</td>\n",
       "      <td>-4.343540</td>\n",
       "      <td>3.255630</td>\n",
       "      <td>168.820698</td>\n",
       "      <td>-3.732810</td>\n",
       "      <td>-3.804264</td>\n",
       "      <td>336.616804</td>\n",
       "      <td>921</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>159.789993</td>\n",
       "      <td>155.380005</td>\n",
       "      <td>155.910004</td>\n",
       "      <td>156.570007</td>\n",
       "      <td>88063200.0</td>\n",
       "      <td>156.570007</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.963265</td>\n",
       "      <td>-0.543871</td>\n",
       "      <td>-3.583606</td>\n",
       "      <td>3.039735</td>\n",
       "      <td>167.321938</td>\n",
       "      <td>-0.146681</td>\n",
       "      <td>-0.146789</td>\n",
       "      <td>335.976370</td>\n",
       "      <td>1158</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>164.520004</td>\n",
       "      <td>158.929993</td>\n",
       "      <td>159.250000</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>115586400.0</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693012</td>\n",
       "      <td>2.420640</td>\n",
       "      <td>-2.382757</td>\n",
       "      <td>4.803397</td>\n",
       "      <td>167.157618</td>\n",
       "      <td>4.515547</td>\n",
       "      <td>4.416565</td>\n",
       "      <td>355.663088</td>\n",
       "      <td>3489</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close       Volume  \\\n",
       "Date                                                                      \n",
       "2022-01-03  182.880005  177.710007  177.830002  182.009995  104487900.0   \n",
       "2022-01-04  182.940002  179.119995  182.630005  179.699997   99310400.0   \n",
       "2022-01-05  180.169998  174.639999  179.610001  174.919998   94537600.0   \n",
       "2022-01-06  175.300003  171.639999  172.699997  172.000000   96904000.0   \n",
       "2022-01-07  174.139999  171.029999  172.889999  172.169998   86709100.0   \n",
       "...                ...         ...         ...         ...          ...   \n",
       "2022-04-22  167.869995  161.500000  166.460007  161.789993   84775200.0   \n",
       "2022-04-25  163.169998  158.460007  161.119995  162.880005   96046400.0   \n",
       "2022-04-26  162.339996  156.720001  162.250000  156.800003   95623200.0   \n",
       "2022-04-27  159.789993  155.380005  155.910004  156.570007   88063200.0   \n",
       "2022-04-28  164.520004  158.929993  159.250000  163.639999  115586400.0   \n",
       "\n",
       "             Adj Close  Year  Month  Week  Day  ...  momentum_ppo_hist  \\\n",
       "Date                                            ...                      \n",
       "2022-01-03  181.778397  2022      1     1    3  ...          -0.143274   \n",
       "2022-01-04  179.471344  2022      1     1    4  ...          -0.189433   \n",
       "2022-01-05  174.697418  2022      1     1    5  ...          -0.407488   \n",
       "2022-01-06  171.781143  2022      1     1    6  ...          -0.653812   \n",
       "2022-01-07  171.950928  2022      1     1    7  ...          -0.791825   \n",
       "...                ...   ...    ...   ...  ...  ...                ...   \n",
       "2022-04-22  161.789993  2022      4    16   22  ...          -0.658369   \n",
       "2022-04-25  162.880005  2022      4    17   25  ...          -0.666083   \n",
       "2022-04-26  156.800003  2022      4    17   26  ...          -0.870720   \n",
       "2022-04-27  156.570007  2022      4    17   27  ...          -0.963265   \n",
       "2022-04-28  163.639999  2022      4    17   28  ...          -0.693012   \n",
       "\n",
       "            momentum_pvo  momentum_pvo_signal  momentum_pvo_hist  \\\n",
       "Date                                                               \n",
       "2022-01-03     -8.251705            -2.942693          -5.309012   \n",
       "2022-01-04     -6.848703            -3.723895          -3.124807   \n",
       "2022-01-05     -6.083336            -4.195783          -1.887553   \n",
       "2022-01-06     -5.210295            -4.398686          -0.811609   \n",
       "2022-01-07     -5.341759            -4.587300          -0.754458   \n",
       "...                  ...                  ...                ...   \n",
       "2022-04-22     -4.834595            -5.769592           0.934997   \n",
       "2022-04-25     -2.708870            -5.157448           2.448578   \n",
       "2022-04-26     -1.087910            -4.343540           3.255630   \n",
       "2022-04-27     -0.543871            -3.583606           3.039735   \n",
       "2022-04-28      2.420640            -2.382757           4.803397   \n",
       "\n",
       "            momentum_kama  others_dr  others_dlr   others_cr  bullish  bearish  \n",
       "Date                                                                            \n",
       "2022-01-03     173.552778   2.500415    2.469666  406.815182        0        0  \n",
       "2022-01-04     174.349880  -1.269160   -1.277282  400.382888        0        0  \n",
       "2022-01-05     174.357792  -2.659988   -2.696006  387.072762      545      195  \n",
       "2022-01-06     174.294544  -1.669334   -1.683424  378.941893      548      170  \n",
       "2022-01-07     174.226442   0.098836    0.098787  379.415261      478      110  \n",
       "...                   ...        ...         ...         ...      ...      ...  \n",
       "2022-04-22     170.303486  -2.782120   -2.821555  350.511661      756      510  \n",
       "2022-04-25     169.837855   0.673720    0.671461  353.546848      951      395  \n",
       "2022-04-26     168.820698  -3.732810   -3.804264  336.616804      921      531  \n",
       "2022-04-27     167.321938  -0.146681   -0.146789  335.976370     1158      391  \n",
       "2022-04-28     167.157618   4.515547    4.416565  355.663088     3489     2315  \n",
       "\n",
       "[81 rows x 106 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_ppo</th>\n",
       "      <th>momentum_ppo_signal</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>momentum_pvo_signal</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-09</th>\n",
       "      <td>40.317501</td>\n",
       "      <td>39.777500</td>\n",
       "      <td>39.814999</td>\n",
       "      <td>40.264999</td>\n",
       "      <td>104526000.0</td>\n",
       "      <td>38.173512</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.965409</td>\n",
       "      <td>1.310218</td>\n",
       "      <td>0.655191</td>\n",
       "      <td>5.497509</td>\n",
       "      <td>3.675388</td>\n",
       "      <td>1.822121</td>\n",
       "      <td>38.576629</td>\n",
       "      <td>0.612191</td>\n",
       "      <td>0.610325</td>\n",
       "      <td>12.119739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-10</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.657501</td>\n",
       "      <td>39.974998</td>\n",
       "      <td>38.830002</td>\n",
       "      <td>163217200.0</td>\n",
       "      <td>36.957611</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.809656</td>\n",
       "      <td>1.410105</td>\n",
       "      <td>0.399550</td>\n",
       "      <td>8.404348</td>\n",
       "      <td>4.621180</td>\n",
       "      <td>3.783168</td>\n",
       "      <td>38.585672</td>\n",
       "      <td>-3.563883</td>\n",
       "      <td>-3.628940</td>\n",
       "      <td>8.123922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>39.642502</td>\n",
       "      <td>39.017502</td>\n",
       "      <td>39.150002</td>\n",
       "      <td>39.369999</td>\n",
       "      <td>105028400.0</td>\n",
       "      <td>37.471577</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1.777848</td>\n",
       "      <td>1.483654</td>\n",
       "      <td>0.294194</td>\n",
       "      <td>6.635481</td>\n",
       "      <td>5.024040</td>\n",
       "      <td>1.611441</td>\n",
       "      <td>38.640159</td>\n",
       "      <td>1.390670</td>\n",
       "      <td>1.381089</td>\n",
       "      <td>9.627569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-14</th>\n",
       "      <td>40.052502</td>\n",
       "      <td>39.687500</td>\n",
       "      <td>39.830002</td>\n",
       "      <td>39.962502</td>\n",
       "      <td>88490800.0</td>\n",
       "      <td>38.035515</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.853388</td>\n",
       "      <td>1.557601</td>\n",
       "      <td>0.295788</td>\n",
       "      <td>4.022824</td>\n",
       "      <td>4.823797</td>\n",
       "      <td>-0.800973</td>\n",
       "      <td>38.779533</td>\n",
       "      <td>1.504960</td>\n",
       "      <td>1.493747</td>\n",
       "      <td>11.277419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-15</th>\n",
       "      <td>40.549999</td>\n",
       "      <td>40.035000</td>\n",
       "      <td>40.165001</td>\n",
       "      <td>40.400002</td>\n",
       "      <td>117862000.0</td>\n",
       "      <td>38.451912</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.979255</td>\n",
       "      <td>1.641932</td>\n",
       "      <td>0.337324</td>\n",
       "      <td>3.959375</td>\n",
       "      <td>4.650913</td>\n",
       "      <td>-0.691537</td>\n",
       "      <td>38.956294</td>\n",
       "      <td>1.094776</td>\n",
       "      <td>1.088827</td>\n",
       "      <td>12.495658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-22</th>\n",
       "      <td>167.869995</td>\n",
       "      <td>161.500000</td>\n",
       "      <td>166.460007</td>\n",
       "      <td>161.789993</td>\n",
       "      <td>84775200.0</td>\n",
       "      <td>161.789993</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502188</td>\n",
       "      <td>0.156180</td>\n",
       "      <td>-0.658369</td>\n",
       "      <td>-4.834595</td>\n",
       "      <td>-5.769592</td>\n",
       "      <td>0.934997</td>\n",
       "      <td>170.303486</td>\n",
       "      <td>-2.782120</td>\n",
       "      <td>-2.821555</td>\n",
       "      <td>350.511661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-25</th>\n",
       "      <td>163.169998</td>\n",
       "      <td>158.460007</td>\n",
       "      <td>161.119995</td>\n",
       "      <td>162.880005</td>\n",
       "      <td>96046400.0</td>\n",
       "      <td>162.880005</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.676423</td>\n",
       "      <td>-0.010340</td>\n",
       "      <td>-0.666083</td>\n",
       "      <td>-2.708870</td>\n",
       "      <td>-5.157448</td>\n",
       "      <td>2.448578</td>\n",
       "      <td>169.837855</td>\n",
       "      <td>0.673720</td>\n",
       "      <td>0.671461</td>\n",
       "      <td>353.546848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-26</th>\n",
       "      <td>162.339996</td>\n",
       "      <td>156.720001</td>\n",
       "      <td>162.250000</td>\n",
       "      <td>156.800003</td>\n",
       "      <td>95623200.0</td>\n",
       "      <td>156.800003</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.098741</td>\n",
       "      <td>-0.228020</td>\n",
       "      <td>-0.870720</td>\n",
       "      <td>-1.087910</td>\n",
       "      <td>-4.343540</td>\n",
       "      <td>3.255630</td>\n",
       "      <td>168.820698</td>\n",
       "      <td>-3.732810</td>\n",
       "      <td>-3.804264</td>\n",
       "      <td>336.616804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>159.789993</td>\n",
       "      <td>155.380005</td>\n",
       "      <td>155.910004</td>\n",
       "      <td>156.570007</td>\n",
       "      <td>88063200.0</td>\n",
       "      <td>156.570007</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.432102</td>\n",
       "      <td>-0.468837</td>\n",
       "      <td>-0.963265</td>\n",
       "      <td>-0.543871</td>\n",
       "      <td>-3.583606</td>\n",
       "      <td>3.039735</td>\n",
       "      <td>167.321938</td>\n",
       "      <td>-0.146681</td>\n",
       "      <td>-0.146789</td>\n",
       "      <td>335.976370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>164.520004</td>\n",
       "      <td>158.929993</td>\n",
       "      <td>159.250000</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>115586400.0</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.335101</td>\n",
       "      <td>-0.642090</td>\n",
       "      <td>-0.693012</td>\n",
       "      <td>2.420640</td>\n",
       "      <td>-2.382757</td>\n",
       "      <td>4.803397</td>\n",
       "      <td>167.157618</td>\n",
       "      <td>4.515547</td>\n",
       "      <td>4.416565</td>\n",
       "      <td>355.663088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1189 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close       Volume  \\\n",
       "Date                                                                      \n",
       "2017-08-09   40.317501   39.777500   39.814999   40.264999  104526000.0   \n",
       "2017-08-10   40.000000   38.657501   39.974998   38.830002  163217200.0   \n",
       "2017-08-11   39.642502   39.017502   39.150002   39.369999  105028400.0   \n",
       "2017-08-14   40.052502   39.687500   39.830002   39.962502   88490800.0   \n",
       "2017-08-15   40.549999   40.035000   40.165001   40.400002  117862000.0   \n",
       "...                ...         ...         ...         ...          ...   \n",
       "2022-04-22  167.869995  161.500000  166.460007  161.789993   84775200.0   \n",
       "2022-04-25  163.169998  158.460007  161.119995  162.880005   96046400.0   \n",
       "2022-04-26  162.339996  156.720001  162.250000  156.800003   95623200.0   \n",
       "2022-04-27  159.789993  155.380005  155.910004  156.570007   88063200.0   \n",
       "2022-04-28  164.520004  158.929993  159.250000  163.639999  115586400.0   \n",
       "\n",
       "             Adj Close  Year  Month  Week  Day  ...  momentum_ppo  \\\n",
       "Date                                            ...                 \n",
       "2017-08-09   38.173512  2017      8    32    9  ...      1.965409   \n",
       "2017-08-10   36.957611  2017      8    32   10  ...      1.809656   \n",
       "2017-08-11   37.471577  2017      8    32   11  ...      1.777848   \n",
       "2017-08-14   38.035515  2017      8    33   14  ...      1.853388   \n",
       "2017-08-15   38.451912  2017      8    33   15  ...      1.979255   \n",
       "...                ...   ...    ...   ...  ...  ...           ...   \n",
       "2022-04-22  161.789993  2022      4    16   22  ...     -0.502188   \n",
       "2022-04-25  162.880005  2022      4    17   25  ...     -0.676423   \n",
       "2022-04-26  156.800003  2022      4    17   26  ...     -1.098741   \n",
       "2022-04-27  156.570007  2022      4    17   27  ...     -1.432102   \n",
       "2022-04-28  163.639999  2022      4    17   28  ...     -1.335101   \n",
       "\n",
       "            momentum_ppo_signal  momentum_ppo_hist  momentum_pvo  \\\n",
       "Date                                                               \n",
       "2017-08-09             1.310218           0.655191      5.497509   \n",
       "2017-08-10             1.410105           0.399550      8.404348   \n",
       "2017-08-11             1.483654           0.294194      6.635481   \n",
       "2017-08-14             1.557601           0.295788      4.022824   \n",
       "2017-08-15             1.641932           0.337324      3.959375   \n",
       "...                         ...                ...           ...   \n",
       "2022-04-22             0.156180          -0.658369     -4.834595   \n",
       "2022-04-25            -0.010340          -0.666083     -2.708870   \n",
       "2022-04-26            -0.228020          -0.870720     -1.087910   \n",
       "2022-04-27            -0.468837          -0.963265     -0.543871   \n",
       "2022-04-28            -0.642090          -0.693012      2.420640   \n",
       "\n",
       "            momentum_pvo_signal  momentum_pvo_hist  momentum_kama  others_dr  \\\n",
       "Date                                                                           \n",
       "2017-08-09             3.675388           1.822121      38.576629   0.612191   \n",
       "2017-08-10             4.621180           3.783168      38.585672  -3.563883   \n",
       "2017-08-11             5.024040           1.611441      38.640159   1.390670   \n",
       "2017-08-14             4.823797          -0.800973      38.779533   1.504960   \n",
       "2017-08-15             4.650913          -0.691537      38.956294   1.094776   \n",
       "...                         ...                ...            ...        ...   \n",
       "2022-04-22            -5.769592           0.934997     170.303486  -2.782120   \n",
       "2022-04-25            -5.157448           2.448578     169.837855   0.673720   \n",
       "2022-04-26            -4.343540           3.255630     168.820698  -3.732810   \n",
       "2022-04-27            -3.583606           3.039735     167.321938  -0.146681   \n",
       "2022-04-28            -2.382757           4.803397     167.157618   4.515547   \n",
       "\n",
       "            others_dlr   others_cr  \n",
       "Date                                \n",
       "2017-08-09    0.610325   12.119739  \n",
       "2017-08-10   -3.628940    8.123922  \n",
       "2017-08-11    1.381089    9.627569  \n",
       "2017-08-14    1.493747   11.277419  \n",
       "2017-08-15    1.088827   12.495658  \n",
       "...                ...         ...  \n",
       "2022-04-22   -2.821555  350.511661  \n",
       "2022-04-25    0.671461  353.546848  \n",
       "2022-04-26   -3.804264  336.616804  \n",
       "2022-04-27   -0.146789  335.976370  \n",
       "2022-04-28    4.416565  355.663088  \n",
       "\n",
       "[1189 rows x 104 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>trend_mass_index</th>\n",
       "      <th>trend_vortex_ind_pos</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>trend_sma_fast</th>\n",
       "      <th>momentum_stoch_signal</th>\n",
       "      <th>trend_dpo</th>\n",
       "      <th>volume_vwap</th>\n",
       "      <th>volume_nvi</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>...</th>\n",
       "      <th>volatility_ui</th>\n",
       "      <th>trend_sma_slow</th>\n",
       "      <th>trend_vortex_ind_neg</th>\n",
       "      <th>trend_kst_sig</th>\n",
       "      <th>bullish</th>\n",
       "      <th>bearish</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>volatility_bbw</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>-8.251705</td>\n",
       "      <td>25.725812</td>\n",
       "      <td>1.011669</td>\n",
       "      <td>-5.309012</td>\n",
       "      <td>176.236666</td>\n",
       "      <td>78.828218</td>\n",
       "      <td>-3.230006</td>\n",
       "      <td>175.442399</td>\n",
       "      <td>7115.616430</td>\n",
       "      <td>6.564038e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.669928</td>\n",
       "      <td>172.404616</td>\n",
       "      <td>0.969559</td>\n",
       "      <td>118.009490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095208</td>\n",
       "      <td>-0.143274</td>\n",
       "      <td>9.273015</td>\n",
       "      <td>182.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>-6.848703</td>\n",
       "      <td>25.634624</td>\n",
       "      <td>1.094392</td>\n",
       "      <td>-3.124807</td>\n",
       "      <td>176.856667</td>\n",
       "      <td>80.781268</td>\n",
       "      <td>-5.069000</td>\n",
       "      <td>175.881553</td>\n",
       "      <td>7025.307892</td>\n",
       "      <td>6.494884e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.581105</td>\n",
       "      <td>173.285000</td>\n",
       "      <td>0.891153</td>\n",
       "      <td>115.975285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084438</td>\n",
       "      <td>-0.189433</td>\n",
       "      <td>7.780912</td>\n",
       "      <td>179.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>-6.083336</td>\n",
       "      <td>25.531909</td>\n",
       "      <td>1.015064</td>\n",
       "      <td>-1.887553</td>\n",
       "      <td>177.171666</td>\n",
       "      <td>73.872936</td>\n",
       "      <td>-6.646000</td>\n",
       "      <td>175.817775</td>\n",
       "      <td>6838.435528</td>\n",
       "      <td>6.409920e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.783068</td>\n",
       "      <td>173.849615</td>\n",
       "      <td>0.969154</td>\n",
       "      <td>113.793983</td>\n",
       "      <td>545</td>\n",
       "      <td>195</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>-0.407488</td>\n",
       "      <td>7.359295</td>\n",
       "      <td>174.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>-5.210295</td>\n",
       "      <td>25.348191</td>\n",
       "      <td>0.987762</td>\n",
       "      <td>-0.811609</td>\n",
       "      <td>177.359166</td>\n",
       "      <td>52.196343</td>\n",
       "      <td>-3.251994</td>\n",
       "      <td>175.732804</td>\n",
       "      <td>6838.435528</td>\n",
       "      <td>6.332079e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.959604</td>\n",
       "      <td>174.107307</td>\n",
       "      <td>1.097695</td>\n",
       "      <td>110.848332</td>\n",
       "      <td>548</td>\n",
       "      <td>170</td>\n",
       "      <td>-0.072365</td>\n",
       "      <td>-0.653812</td>\n",
       "      <td>7.659172</td>\n",
       "      <td>172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>-5.341759</td>\n",
       "      <td>25.079790</td>\n",
       "      <td>0.996898</td>\n",
       "      <td>-0.754458</td>\n",
       "      <td>177.290833</td>\n",
       "      <td>35.981871</td>\n",
       "      <td>-0.482500</td>\n",
       "      <td>176.202866</td>\n",
       "      <td>6845.194374</td>\n",
       "      <td>6.308938e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.052086</td>\n",
       "      <td>174.391923</td>\n",
       "      <td>0.964433</td>\n",
       "      <td>107.146247</td>\n",
       "      <td>478</td>\n",
       "      <td>110</td>\n",
       "      <td>-0.055469</td>\n",
       "      <td>-0.791825</td>\n",
       "      <td>7.887688</td>\n",
       "      <td>172.169998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-22</th>\n",
       "      <td>-4.834595</td>\n",
       "      <td>24.250758</td>\n",
       "      <td>0.792892</td>\n",
       "      <td>0.934997</td>\n",
       "      <td>167.589165</td>\n",
       "      <td>15.113140</td>\n",
       "      <td>0.803002</td>\n",
       "      <td>169.248558</td>\n",
       "      <td>7803.812186</td>\n",
       "      <td>6.774359e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.010746</td>\n",
       "      <td>170.139231</td>\n",
       "      <td>1.049587</td>\n",
       "      <td>39.461044</td>\n",
       "      <td>756</td>\n",
       "      <td>510</td>\n",
       "      <td>-0.023885</td>\n",
       "      <td>-0.658369</td>\n",
       "      <td>11.489273</td>\n",
       "      <td>161.789993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-25</th>\n",
       "      <td>-2.708870</td>\n",
       "      <td>24.390685</td>\n",
       "      <td>0.706046</td>\n",
       "      <td>2.448578</td>\n",
       "      <td>166.843332</td>\n",
       "      <td>14.362295</td>\n",
       "      <td>1.705000</td>\n",
       "      <td>168.009840</td>\n",
       "      <td>7803.812186</td>\n",
       "      <td>6.858578e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.291326</td>\n",
       "      <td>170.226155</td>\n",
       "      <td>1.187449</td>\n",
       "      <td>33.289143</td>\n",
       "      <td>951</td>\n",
       "      <td>395</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>-0.666083</td>\n",
       "      <td>12.062967</td>\n",
       "      <td>162.880005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-26</th>\n",
       "      <td>-1.087910</td>\n",
       "      <td>24.584276</td>\n",
       "      <td>0.682444</td>\n",
       "      <td>3.255630</td>\n",
       "      <td>165.564999</td>\n",
       "      <td>8.152724</td>\n",
       "      <td>0.594997</td>\n",
       "      <td>166.676075</td>\n",
       "      <td>7512.510670</td>\n",
       "      <td>6.765678e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.701780</td>\n",
       "      <td>169.950001</td>\n",
       "      <td>1.184898</td>\n",
       "      <td>26.559597</td>\n",
       "      <td>921</td>\n",
       "      <td>531</td>\n",
       "      <td>-0.107833</td>\n",
       "      <td>-0.870720</td>\n",
       "      <td>13.658505</td>\n",
       "      <td>156.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>-0.543871</td>\n",
       "      <td>24.718761</td>\n",
       "      <td>0.724501</td>\n",
       "      <td>3.039735</td>\n",
       "      <td>164.438333</td>\n",
       "      <td>9.789935</td>\n",
       "      <td>-2.625500</td>\n",
       "      <td>165.509576</td>\n",
       "      <td>7501.491248</td>\n",
       "      <td>6.725141e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>7.044409</td>\n",
       "      <td>169.611155</td>\n",
       "      <td>1.175419</td>\n",
       "      <td>18.978894</td>\n",
       "      <td>1158</td>\n",
       "      <td>391</td>\n",
       "      <td>-0.193393</td>\n",
       "      <td>-0.963265</td>\n",
       "      <td>14.276945</td>\n",
       "      <td>156.570007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>2.420640</td>\n",
       "      <td>24.836055</td>\n",
       "      <td>0.764644</td>\n",
       "      <td>4.803397</td>\n",
       "      <td>164.262499</td>\n",
       "      <td>19.152474</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>164.767378</td>\n",
       "      <td>7501.491248</td>\n",
       "      <td>6.804335e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>7.050567</td>\n",
       "      <td>169.411924</td>\n",
       "      <td>1.054107</td>\n",
       "      <td>11.203017</td>\n",
       "      <td>3489</td>\n",
       "      <td>2315</td>\n",
       "      <td>-0.127619</td>\n",
       "      <td>-0.693012</td>\n",
       "      <td>13.563856</td>\n",
       "      <td>163.639999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            momentum_pvo  trend_mass_index  trend_vortex_ind_pos  \\\n",
       "Date                                                               \n",
       "2022-01-03     -8.251705         25.725812              1.011669   \n",
       "2022-01-04     -6.848703         25.634624              1.094392   \n",
       "2022-01-05     -6.083336         25.531909              1.015064   \n",
       "2022-01-06     -5.210295         25.348191              0.987762   \n",
       "2022-01-07     -5.341759         25.079790              0.996898   \n",
       "...                  ...               ...                   ...   \n",
       "2022-04-22     -4.834595         24.250758              0.792892   \n",
       "2022-04-25     -2.708870         24.390685              0.706046   \n",
       "2022-04-26     -1.087910         24.584276              0.682444   \n",
       "2022-04-27     -0.543871         24.718761              0.724501   \n",
       "2022-04-28      2.420640         24.836055              0.764644   \n",
       "\n",
       "            momentum_pvo_hist  trend_sma_fast  momentum_stoch_signal  \\\n",
       "Date                                                                   \n",
       "2022-01-03          -5.309012      176.236666              78.828218   \n",
       "2022-01-04          -3.124807      176.856667              80.781268   \n",
       "2022-01-05          -1.887553      177.171666              73.872936   \n",
       "2022-01-06          -0.811609      177.359166              52.196343   \n",
       "2022-01-07          -0.754458      177.290833              35.981871   \n",
       "...                       ...             ...                    ...   \n",
       "2022-04-22           0.934997      167.589165              15.113140   \n",
       "2022-04-25           2.448578      166.843332              14.362295   \n",
       "2022-04-26           3.255630      165.564999               8.152724   \n",
       "2022-04-27           3.039735      164.438333               9.789935   \n",
       "2022-04-28           4.803397      164.262499              19.152474   \n",
       "\n",
       "            trend_dpo  volume_vwap   volume_nvi    volume_adi  ...  \\\n",
       "Date                                                           ...   \n",
       "2022-01-03  -3.230006   175.442399  7115.616430  6.564038e+09  ...   \n",
       "2022-01-04  -5.069000   175.881553  7025.307892  6.494884e+09  ...   \n",
       "2022-01-05  -6.646000   175.817775  6838.435528  6.409920e+09  ...   \n",
       "2022-01-06  -3.251994   175.732804  6838.435528  6.332079e+09  ...   \n",
       "2022-01-07  -0.482500   176.202866  6845.194374  6.308938e+09  ...   \n",
       "...               ...          ...          ...           ...  ...   \n",
       "2022-04-22   0.803002   169.248558  7803.812186  6.774359e+09  ...   \n",
       "2022-04-25   1.705000   168.009840  7803.812186  6.858578e+09  ...   \n",
       "2022-04-26   0.594997   166.676075  7512.510670  6.765678e+09  ...   \n",
       "2022-04-27  -2.625500   165.509576  7501.491248  6.725141e+09  ...   \n",
       "2022-04-28  -0.008996   164.767378  7501.491248  6.804335e+09  ...   \n",
       "\n",
       "            volatility_ui  trend_sma_slow  trend_vortex_ind_neg  \\\n",
       "Date                                                              \n",
       "2022-01-03       2.669928      172.404616              0.969559   \n",
       "2022-01-04       2.581105      173.285000              0.891153   \n",
       "2022-01-05       2.783068      173.849615              0.969154   \n",
       "2022-01-06       2.959604      174.107307              1.097695   \n",
       "2022-01-07       3.052086      174.391923              0.964433   \n",
       "...                   ...             ...                   ...   \n",
       "2022-04-22       6.010746      170.139231              1.049587   \n",
       "2022-04-25       6.291326      170.226155              1.187449   \n",
       "2022-04-26       6.701780      169.950001              1.184898   \n",
       "2022-04-27       7.044409      169.611155              1.175419   \n",
       "2022-04-28       7.050567      169.411924              1.054107   \n",
       "\n",
       "            trend_kst_sig  bullish  bearish  volume_cmf  momentum_ppo_hist  \\\n",
       "Date                                                                         \n",
       "2022-01-03     118.009490        0        0    0.095208          -0.143274   \n",
       "2022-01-04     115.975285        0        0    0.084438          -0.189433   \n",
       "2022-01-05     113.793983      545      195    0.002168          -0.407488   \n",
       "2022-01-06     110.848332      548      170   -0.072365          -0.653812   \n",
       "2022-01-07     107.146247      478      110   -0.055469          -0.791825   \n",
       "...                   ...      ...      ...         ...                ...   \n",
       "2022-04-22      39.461044      756      510   -0.023885          -0.658369   \n",
       "2022-04-25      33.289143      951      395    0.000494          -0.666083   \n",
       "2022-04-26      26.559597      921      531   -0.107833          -0.870720   \n",
       "2022-04-27      18.978894     1158      391   -0.193393          -0.963265   \n",
       "2022-04-28      11.203017     3489     2315   -0.127619          -0.693012   \n",
       "\n",
       "            volatility_bbw       Close  \n",
       "Date                                    \n",
       "2022-01-03        9.273015  182.009995  \n",
       "2022-01-04        7.780912  179.699997  \n",
       "2022-01-05        7.359295  174.919998  \n",
       "2022-01-06        7.659172  172.000000  \n",
       "2022-01-07        7.887688  172.169998  \n",
       "...                    ...         ...  \n",
       "2022-04-22       11.489273  161.789993  \n",
       "2022-04-25       12.062967  162.880005  \n",
       "2022-04-26       13.658505  156.800003  \n",
       "2022-04-27       14.276945  156.570007  \n",
       "2022-04-28       13.563856  163.639999  \n",
       "\n",
       "[81 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>trend_mass_index</th>\n",
       "      <th>trend_sma_fast</th>\n",
       "      <th>trend_vortex_ind_pos</th>\n",
       "      <th>volume_vwap</th>\n",
       "      <th>volume_nvi</th>\n",
       "      <th>trend_dpo</th>\n",
       "      <th>momentum_stoch_signal</th>\n",
       "      <th>volatility_ui</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>trend_stc</th>\n",
       "      <th>momentum_ao</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volatility_dcp</th>\n",
       "      <th>trend_sma_slow</th>\n",
       "      <th>volatility_bbl</th>\n",
       "      <th>volatility_atr</th>\n",
       "      <th>Day</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>-8.251705</td>\n",
       "      <td>25.725812</td>\n",
       "      <td>176.236666</td>\n",
       "      <td>1.011669</td>\n",
       "      <td>175.442399</td>\n",
       "      <td>7115.616430</td>\n",
       "      <td>-3.230006</td>\n",
       "      <td>78.828218</td>\n",
       "      <td>2.669928</td>\n",
       "      <td>-5.309012</td>\n",
       "      <td>...</td>\n",
       "      <td>6.564038e+09</td>\n",
       "      <td>79.968965</td>\n",
       "      <td>10.892441</td>\n",
       "      <td>0.095208</td>\n",
       "      <td>0.953225</td>\n",
       "      <td>172.404616</td>\n",
       "      <td>167.353393</td>\n",
       "      <td>3.993418</td>\n",
       "      <td>3</td>\n",
       "      <td>182.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>-6.848703</td>\n",
       "      <td>25.634624</td>\n",
       "      <td>176.856667</td>\n",
       "      <td>1.094392</td>\n",
       "      <td>175.881553</td>\n",
       "      <td>7025.307892</td>\n",
       "      <td>-5.069000</td>\n",
       "      <td>80.781268</td>\n",
       "      <td>2.581105</td>\n",
       "      <td>-3.124807</td>\n",
       "      <td>...</td>\n",
       "      <td>6.494884e+09</td>\n",
       "      <td>83.025870</td>\n",
       "      <td>10.219059</td>\n",
       "      <td>0.084438</td>\n",
       "      <td>0.790697</td>\n",
       "      <td>173.285000</td>\n",
       "      <td>169.353666</td>\n",
       "      <td>3.976077</td>\n",
       "      <td>4</td>\n",
       "      <td>179.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>-6.083336</td>\n",
       "      <td>25.531909</td>\n",
       "      <td>177.171666</td>\n",
       "      <td>1.015064</td>\n",
       "      <td>175.817775</td>\n",
       "      <td>6838.435528</td>\n",
       "      <td>-6.646000</td>\n",
       "      <td>73.872936</td>\n",
       "      <td>2.783068</td>\n",
       "      <td>-1.887553</td>\n",
       "      <td>...</td>\n",
       "      <td>6.409920e+09</td>\n",
       "      <td>41.512935</td>\n",
       "      <td>9.029235</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.481912</td>\n",
       "      <td>173.849615</td>\n",
       "      <td>169.905249</td>\n",
       "      <td>4.131469</td>\n",
       "      <td>5</td>\n",
       "      <td>174.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>-5.210295</td>\n",
       "      <td>25.348191</td>\n",
       "      <td>177.359166</td>\n",
       "      <td>0.987762</td>\n",
       "      <td>175.732804</td>\n",
       "      <td>6838.435528</td>\n",
       "      <td>-3.251994</td>\n",
       "      <td>52.196343</td>\n",
       "      <td>2.959604</td>\n",
       "      <td>-0.811609</td>\n",
       "      <td>...</td>\n",
       "      <td>6.332079e+09</td>\n",
       "      <td>20.756467</td>\n",
       "      <td>7.255029</td>\n",
       "      <td>-0.072365</td>\n",
       "      <td>0.293281</td>\n",
       "      <td>174.107307</td>\n",
       "      <td>169.492661</td>\n",
       "      <td>4.084322</td>\n",
       "      <td>6</td>\n",
       "      <td>172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>-5.341759</td>\n",
       "      <td>25.079790</td>\n",
       "      <td>177.290833</td>\n",
       "      <td>0.996898</td>\n",
       "      <td>176.202866</td>\n",
       "      <td>6845.194374</td>\n",
       "      <td>-0.482500</td>\n",
       "      <td>35.981871</td>\n",
       "      <td>3.052086</td>\n",
       "      <td>-0.754458</td>\n",
       "      <td>...</td>\n",
       "      <td>6.308938e+09</td>\n",
       "      <td>10.378234</td>\n",
       "      <td>5.631118</td>\n",
       "      <td>-0.055469</td>\n",
       "      <td>0.304263</td>\n",
       "      <td>174.391923</td>\n",
       "      <td>169.176504</td>\n",
       "      <td>3.986890</td>\n",
       "      <td>7</td>\n",
       "      <td>172.169998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-22</th>\n",
       "      <td>-4.834595</td>\n",
       "      <td>24.250758</td>\n",
       "      <td>167.589165</td>\n",
       "      <td>0.792892</td>\n",
       "      <td>169.248558</td>\n",
       "      <td>7803.812186</td>\n",
       "      <td>0.803002</td>\n",
       "      <td>15.113140</td>\n",
       "      <td>6.010746</td>\n",
       "      <td>0.934997</td>\n",
       "      <td>...</td>\n",
       "      <td>6.774359e+09</td>\n",
       "      <td>0.112911</td>\n",
       "      <td>-0.699527</td>\n",
       "      <td>-0.023885</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>170.139231</td>\n",
       "      <td>161.202121</td>\n",
       "      <td>4.367901</td>\n",
       "      <td>22</td>\n",
       "      <td>161.789993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-25</th>\n",
       "      <td>-2.708870</td>\n",
       "      <td>24.390685</td>\n",
       "      <td>166.843332</td>\n",
       "      <td>0.706046</td>\n",
       "      <td>168.009840</td>\n",
       "      <td>7803.812186</td>\n",
       "      <td>1.705000</td>\n",
       "      <td>14.362295</td>\n",
       "      <td>6.291326</td>\n",
       "      <td>2.448578</td>\n",
       "      <td>...</td>\n",
       "      <td>6.858578e+09</td>\n",
       "      <td>0.056456</td>\n",
       "      <td>-1.517793</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>170.226155</td>\n",
       "      <td>160.155241</td>\n",
       "      <td>4.402110</td>\n",
       "      <td>25</td>\n",
       "      <td>162.880005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-26</th>\n",
       "      <td>-1.087910</td>\n",
       "      <td>24.584276</td>\n",
       "      <td>165.564999</td>\n",
       "      <td>0.682444</td>\n",
       "      <td>166.676075</td>\n",
       "      <td>7512.510670</td>\n",
       "      <td>0.594997</td>\n",
       "      <td>8.152724</td>\n",
       "      <td>6.701780</td>\n",
       "      <td>3.255630</td>\n",
       "      <td>...</td>\n",
       "      <td>6.765678e+09</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>-2.790382</td>\n",
       "      <td>-0.107833</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>169.950001</td>\n",
       "      <td>157.919758</td>\n",
       "      <td>4.577899</td>\n",
       "      <td>26</td>\n",
       "      <td>156.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>-0.543871</td>\n",
       "      <td>24.718761</td>\n",
       "      <td>164.438333</td>\n",
       "      <td>0.724501</td>\n",
       "      <td>165.509576</td>\n",
       "      <td>7501.491248</td>\n",
       "      <td>-2.625500</td>\n",
       "      <td>9.789935</td>\n",
       "      <td>7.044409</td>\n",
       "      <td>3.039735</td>\n",
       "      <td>...</td>\n",
       "      <td>6.725141e+09</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>-4.658883</td>\n",
       "      <td>-0.193393</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>169.611155</td>\n",
       "      <td>156.356061</td>\n",
       "      <td>4.561108</td>\n",
       "      <td>27</td>\n",
       "      <td>156.570007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>2.420640</td>\n",
       "      <td>24.836055</td>\n",
       "      <td>164.262499</td>\n",
       "      <td>0.764644</td>\n",
       "      <td>164.767378</td>\n",
       "      <td>7501.491248</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>19.152474</td>\n",
       "      <td>7.050567</td>\n",
       "      <td>4.803397</td>\n",
       "      <td>...</td>\n",
       "      <td>6.804335e+09</td>\n",
       "      <td>0.007057</td>\n",
       "      <td>-6.162001</td>\n",
       "      <td>-0.127619</td>\n",
       "      <td>0.357421</td>\n",
       "      <td>169.411924</td>\n",
       "      <td>156.297808</td>\n",
       "      <td>4.899997</td>\n",
       "      <td>28</td>\n",
       "      <td>163.639999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            momentum_pvo  trend_mass_index  trend_sma_fast  \\\n",
       "Date                                                         \n",
       "2022-01-03     -8.251705         25.725812      176.236666   \n",
       "2022-01-04     -6.848703         25.634624      176.856667   \n",
       "2022-01-05     -6.083336         25.531909      177.171666   \n",
       "2022-01-06     -5.210295         25.348191      177.359166   \n",
       "2022-01-07     -5.341759         25.079790      177.290833   \n",
       "...                  ...               ...             ...   \n",
       "2022-04-22     -4.834595         24.250758      167.589165   \n",
       "2022-04-25     -2.708870         24.390685      166.843332   \n",
       "2022-04-26     -1.087910         24.584276      165.564999   \n",
       "2022-04-27     -0.543871         24.718761      164.438333   \n",
       "2022-04-28      2.420640         24.836055      164.262499   \n",
       "\n",
       "            trend_vortex_ind_pos  volume_vwap   volume_nvi  trend_dpo  \\\n",
       "Date                                                                    \n",
       "2022-01-03              1.011669   175.442399  7115.616430  -3.230006   \n",
       "2022-01-04              1.094392   175.881553  7025.307892  -5.069000   \n",
       "2022-01-05              1.015064   175.817775  6838.435528  -6.646000   \n",
       "2022-01-06              0.987762   175.732804  6838.435528  -3.251994   \n",
       "2022-01-07              0.996898   176.202866  6845.194374  -0.482500   \n",
       "...                          ...          ...          ...        ...   \n",
       "2022-04-22              0.792892   169.248558  7803.812186   0.803002   \n",
       "2022-04-25              0.706046   168.009840  7803.812186   1.705000   \n",
       "2022-04-26              0.682444   166.676075  7512.510670   0.594997   \n",
       "2022-04-27              0.724501   165.509576  7501.491248  -2.625500   \n",
       "2022-04-28              0.764644   164.767378  7501.491248  -0.008996   \n",
       "\n",
       "            momentum_stoch_signal  volatility_ui  momentum_pvo_hist  ...  \\\n",
       "Date                                                                 ...   \n",
       "2022-01-03              78.828218       2.669928          -5.309012  ...   \n",
       "2022-01-04              80.781268       2.581105          -3.124807  ...   \n",
       "2022-01-05              73.872936       2.783068          -1.887553  ...   \n",
       "2022-01-06              52.196343       2.959604          -0.811609  ...   \n",
       "2022-01-07              35.981871       3.052086          -0.754458  ...   \n",
       "...                           ...            ...                ...  ...   \n",
       "2022-04-22              15.113140       6.010746           0.934997  ...   \n",
       "2022-04-25              14.362295       6.291326           2.448578  ...   \n",
       "2022-04-26               8.152724       6.701780           3.255630  ...   \n",
       "2022-04-27               9.789935       7.044409           3.039735  ...   \n",
       "2022-04-28              19.152474       7.050567           4.803397  ...   \n",
       "\n",
       "              volume_adi  trend_stc  momentum_ao  volume_cmf  volatility_dcp  \\\n",
       "Date                                                                           \n",
       "2022-01-03  6.564038e+09  79.968965    10.892441    0.095208        0.953225   \n",
       "2022-01-04  6.494884e+09  83.025870    10.219059    0.084438        0.790697   \n",
       "2022-01-05  6.409920e+09  41.512935     9.029235    0.002168        0.481912   \n",
       "2022-01-06  6.332079e+09  20.756467     7.255029   -0.072365        0.293281   \n",
       "2022-01-07  6.308938e+09  10.378234     5.631118   -0.055469        0.304263   \n",
       "...                  ...        ...          ...         ...             ...   \n",
       "2022-04-22  6.774359e+09   0.112911    -0.699527   -0.023885        0.016013   \n",
       "2022-04-25  6.858578e+09   0.056456    -1.517793    0.000494        0.208983   \n",
       "2022-04-26  6.765678e+09   0.028228    -2.790382   -0.107833        0.003495   \n",
       "2022-04-27  6.725141e+09   0.014114    -4.658883   -0.193393        0.049113   \n",
       "2022-04-28  6.804335e+09   0.007057    -6.162001   -0.127619        0.357421   \n",
       "\n",
       "            trend_sma_slow  volatility_bbl  volatility_atr  Day       Close  \n",
       "Date                                                                         \n",
       "2022-01-03      172.404616      167.353393        3.993418    3  182.009995  \n",
       "2022-01-04      173.285000      169.353666        3.976077    4  179.699997  \n",
       "2022-01-05      173.849615      169.905249        4.131469    5  174.919998  \n",
       "2022-01-06      174.107307      169.492661        4.084322    6  172.000000  \n",
       "2022-01-07      174.391923      169.176504        3.986890    7  172.169998  \n",
       "...                    ...             ...             ...  ...         ...  \n",
       "2022-04-22      170.139231      161.202121        4.367901   22  161.789993  \n",
       "2022-04-25      170.226155      160.155241        4.402110   25  162.880005  \n",
       "2022-04-26      169.950001      157.919758        4.577899   26  156.800003  \n",
       "2022-04-27      169.611155      156.356061        4.561108   27  156.570007  \n",
       "2022-04-28      169.411924      156.297808        4.899997   28  163.639999  \n",
       "\n",
       "[81 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_noSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study with long-term data and all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = convert_df(df_all)\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([951, 100]) torch.Size([951, 1]) torch.Size([238, 100]) torch.Size([238, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lt, hs_lt, nl_lt, dp_lt, study_lt = optimize_hyperparameters(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study with short-term data and all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = convert_df(df_all_short)\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 102]) torch.Size([64, 1]) torch.Size([17, 102]) torch.Size([17, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sta, hs_sta, nl_sta, dp_sta, study_sta = optimize_hyperparameters(df_all_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"learning rate: \" + str(lr_sta))\n",
    "print(\"hidden size: \" + str(hs_sta))\n",
    "print(\"number of layers: \" + str(nl_sta))\n",
    "print(\"dropout rate: \" + str(dp_sta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study with short-term data and selected parameters, including StockTwits sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = convert_df(df_selected_sentiment)\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20]) torch.Size([64, 1]) torch.Size([17, 20]) torch.Size([17, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sts, hs_sts, nl_sts, dp_sts, study_sts = optimize_hyperparameters(df_selected_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"learning rate: \" + str(lr_sts))\n",
    "print(\"hidden size: \" + str(hs_sts))\n",
    "print(\"number of layers: \" + str(nl_sts))\n",
    "print(\"dropout rate: \" + str(dp_sts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study with short-term data and selected parameters, excluding StockTwits sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = convert_df(df_selected_noSentiment)\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20]) torch.Size([64, 1]) torch.Size([17, 20]) torch.Size([17, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-01 16:01:07,457]\u001b[0m A new study created in memory with name: no-name-a6ffbcef-6540-45e9-ad1c-6469d2b303f9\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:06:50,393]\u001b[0m Trial 0 finished with value: 1.770090913772583 and parameters: {'learning_rate': 0.008825182499014803, 'hidden_size': 80, 'num_layers': 4, 'dropout_p': 0.2}. Best is trial 0 with value: 1.770090913772583.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:09:06,453]\u001b[0m Trial 1 finished with value: 1.9544145345687867 and parameters: {'learning_rate': 0.0009991500382845982, 'hidden_size': 80, 'num_layers': 2, 'dropout_p': 0.1}. Best is trial 0 with value: 1.770090913772583.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:12:35,485]\u001b[0m Trial 2 finished with value: 1.7041390419006348 and parameters: {'learning_rate': 0.0012883546479614287, 'hidden_size': 60, 'num_layers': 4, 'dropout_p': 0.2}. Best is trial 2 with value: 1.7041390419006348.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:14:25,721]\u001b[0m Trial 3 finished with value: 1.7596633672714233 and parameters: {'learning_rate': 0.055358243077249424, 'hidden_size': 60, 'num_layers': 2, 'dropout_p': 0.0}. Best is trial 2 with value: 1.7041390419006348.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:18:53,449]\u001b[0m Trial 4 finished with value: 1.128068208694458 and parameters: {'learning_rate': 0.037117428013245295, 'hidden_size': 80, 'num_layers': 4, 'dropout_p': 0.2}. Best is trial 4 with value: 1.128068208694458.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:22:28,422]\u001b[0m Trial 5 finished with value: 1.046261990070343 and parameters: {'learning_rate': 0.044904110434606646, 'hidden_size': 80, 'num_layers': 4, 'dropout_p': 0.0}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:24:38,394]\u001b[0m Trial 6 finished with value: 1.7308997631072998 and parameters: {'learning_rate': 0.0022875020640933242, 'hidden_size': 40, 'num_layers': 4, 'dropout_p': 0.0}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:27:21,330]\u001b[0m Trial 7 finished with value: 1.3002699375152589 and parameters: {'learning_rate': 0.024682765497338305, 'hidden_size': 60, 'num_layers': 4, 'dropout_p': 0.0}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:29:26,263]\u001b[0m Trial 8 finished with value: 1.7758496761322022 and parameters: {'learning_rate': 0.024011869458498997, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.2}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:30:56,479]\u001b[0m Trial 9 finished with value: 1.8105393171310424 and parameters: {'learning_rate': 0.0007030021188346967, 'hidden_size': 60, 'num_layers': 2, 'dropout_p': 0.1}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:33:36,063]\u001b[0m Trial 10 finished with value: 2.0595726013183593 and parameters: {'learning_rate': 3.218854799189005e-05, 'hidden_size': 100, 'num_layers': 3, 'dropout_p': 0.0}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:37:33,334]\u001b[0m Trial 11 finished with value: 1.2057945013046265 and parameters: {'learning_rate': 0.06692375969548547, 'hidden_size': 100, 'num_layers': 3, 'dropout_p': 0.1}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:40:58,366]\u001b[0m Trial 12 finished with value: 1.8755425453186034 and parameters: {'learning_rate': 0.006762756895688552, 'hidden_size': 80, 'num_layers': 4, 'dropout_p': 0.2}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:44:33,297]\u001b[0m Trial 13 finished with value: 1.22805198431015 and parameters: {'learning_rate': 8.960981073493188e-05, 'hidden_size': 100, 'num_layers': 4, 'dropout_p': 0.1}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:47:06,625]\u001b[0m Trial 14 finished with value: 1.7322660446166993 and parameters: {'learning_rate': 0.00020994449001626438, 'hidden_size': 80, 'num_layers': 3, 'dropout_p': 0.1}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:50:31,346]\u001b[0m Trial 15 finished with value: 1.7322003364562988 and parameters: {'learning_rate': 0.007343960083670797, 'hidden_size': 80, 'num_layers': 4, 'dropout_p': 0.0}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:54:27,467]\u001b[0m Trial 16 finished with value: 1.2602493405342101 and parameters: {'learning_rate': 0.08200445818718129, 'hidden_size': 100, 'num_layers': 3, 'dropout_p': 0.2}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 16:57:39,280]\u001b[0m Trial 17 finished with value: 1.9909196376800538 and parameters: {'learning_rate': 0.01790864601417463, 'hidden_size': 80, 'num_layers': 4, 'dropout_p': 0.1}. Best is trial 5 with value: 1.046261990070343.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:00:02,001]\u001b[0m Trial 18 finished with value: 1.0268841862678528 and parameters: {'learning_rate': 1.2342345882815991e-05, 'hidden_size': 60, 'num_layers': 3, 'dropout_p': 0.1}. Best is trial 18 with value: 1.0268841862678528.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:02:06,989]\u001b[0m Trial 19 finished with value: 1.0190691947937012 and parameters: {'learning_rate': 1.6308237250911114e-05, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.0}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:04:00,446]\u001b[0m Trial 20 finished with value: 1.0342864394187927 and parameters: {'learning_rate': 1.1226039804227669e-05, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.1}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:06:08,981]\u001b[0m Trial 21 finished with value: 1.0201854348182677 and parameters: {'learning_rate': 1.4702958952113481e-05, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.1}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:08:35,612]\u001b[0m Trial 22 finished with value: 1.0274357438087462 and parameters: {'learning_rate': 1.1753942036552405e-05, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.1}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:11:09,494]\u001b[0m Trial 23 finished with value: 1.1594365000724793 and parameters: {'learning_rate': 3.565034535538934e-05, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.1}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:13:39,399]\u001b[0m Trial 24 finished with value: 1.2797531843185426 and parameters: {'learning_rate': 3.6048219202208804e-05, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.0}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:15:46,921]\u001b[0m Trial 25 finished with value: 2.3996270656585694 and parameters: {'learning_rate': 0.00015647977634652825, 'hidden_size': 60, 'num_layers': 2, 'dropout_p': 0.1}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:18:27,150]\u001b[0m Trial 26 finished with value: 1.054485595226288 and parameters: {'learning_rate': 2.283629966722221e-05, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.0}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:20:16,750]\u001b[0m Trial 27 finished with value: 1.6850581169128418 and parameters: {'learning_rate': 6.353191739263004e-05, 'hidden_size': 60, 'num_layers': 2, 'dropout_p': 0.1}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:22:35,819]\u001b[0m Trial 28 finished with value: 1.6016302585601807 and parameters: {'learning_rate': 0.00024501151900095983, 'hidden_size': 40, 'num_layers': 3, 'dropout_p': 0.0}. Best is trial 19 with value: 1.0190691947937012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-01 17:25:09,302]\u001b[0m Trial 29 finished with value: 1.0095340728759765 and parameters: {'learning_rate': 1.7691961498826596e-05, 'hidden_size': 60, 'num_layers': 3, 'dropout_p': 0.2}. Best is trial 29 with value: 1.0095340728759765.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lr_stns, hs_stns, nl_stns, dp_stns, study_stns = optimize_hyperparameters(df_selected_noSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.7691961498826596e-05\n",
      "hidden size: 60\n",
      "number of layers: 3\n",
      "dropout rate: 0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"learning rate: \" + str(lr_stns))\n",
    "print(\"hidden size: \" + str(hs_stns))\n",
    "print(\"number of layers: \" + str(nl_stns))\n",
    "print(\"dropout rate: \" + str(dp_stns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute obtained HP to avoid HP optimization, which takes a long time\n",
    "    # long-term data and all parameters\n",
    "lr_lta =  1.1883247534334845e-05\n",
    "hs_lta =  80\n",
    "nl_lta =  4\n",
    "dp_lta =  0.1\n",
    "    # short-term data and all parameters\n",
    "lr_sta =  1.1883247534334845e-05\n",
    "hs_sta =  80\n",
    "nl_sta =  4\n",
    "dp_sta =  0.1\n",
    "   # short-term data and selected parameters, including StockTwits sentiment \n",
    "lr_sts = 1.1615177249554967e-05\n",
    "hs_sts = 100\n",
    "nl_sts =  3\n",
    "dp_sts =  0.2\n",
    "    # short-term data and selected parameters, excluding StockTwits sentiment\n",
    "lr_stns = 1.7691961498826596e-05\n",
    "hs_stns = 60\n",
    "nl_stns = 3\n",
    "dp_stns = 0.2\n",
    "\n",
    "\n",
    "# Scale data\n",
    "    # long-term data and all parameters\n",
    "x_train_lta, y_train_lta, x_test_lta, y_test_lta = convert_df(df_all)\n",
    "\n",
    "t_scaler_lta = MinMaxScaler()\n",
    "x_train_lta = t_scaler_lta.fit_transform(x_train_lta)\n",
    "x_test_lta = t_scaler_lta.transform(x_test_lta)\n",
    "\n",
    "x_train_lta = torch.tensor(x_train_lta, dtype=torch.float32)\n",
    "x_test_lta = torch.tensor(x_test_lta, dtype=torch.float32)\n",
    "\n",
    "train_data_lta = (x_train_lta, y_train_lta)\n",
    "\n",
    "    # short-term data and all parameters\n",
    "x_train_sta, y_train_sta, x_test_sta, y_test_sta = convert_df(df_all_short)\n",
    "\n",
    "t_scaler_sta = MinMaxScaler()\n",
    "x_train_sta = t_scaler_sta.fit_transform(x_train_sta)\n",
    "x_test_sta = t_scaler_sta.transform(x_test_sta)\n",
    "\n",
    "x_train_sta = torch.tensor(x_train_sta, dtype=torch.float32)\n",
    "x_test_sta = torch.tensor(x_test_sta, dtype=torch.float32)\n",
    "\n",
    "train_data_sta = (x_train_sta, y_train_sta)\n",
    "\n",
    "   # short-term data and selected parameters, including StockTwits sentiment \n",
    "x_train_sts, y_train_sts, x_test_sts, y_test_sts = convert_df(df_selected_sentiment)\n",
    "\n",
    "t_scaler_sts = MinMaxScaler()\n",
    "x_train_sts = t_scaler_sts.fit_transform(x_train_sts)\n",
    "x_test_sts = t_scaler_sts.transform(x_test_sts)\n",
    "\n",
    "x_train_sts = torch.tensor(x_train_sts, dtype=torch.float32)\n",
    "x_test_sts = torch.tensor(x_test_sts, dtype=torch.float32)\n",
    "\n",
    "train_data_sts = (x_train_sts, y_train_sts)\n",
    "\n",
    "    # short-term data and selected parameters, excluding StockTwits sentiment\n",
    "x_train_stns, y_train_stns, x_test_stns, y_test_stns = convert_df(df_selected_noSentiment)\n",
    "\n",
    "t_scaler_stns = MinMaxScaler()\n",
    "x_train_stns = t_scaler_stns.fit_transform(x_train_stns)\n",
    "x_test_stns = t_scaler_stns.transform(x_test_stns)\n",
    "\n",
    "x_train_stns = torch.tensor(x_train_stns, dtype=torch.float32)\n",
    "x_test_stns = torch.tensor(x_test_stns, dtype=torch.float32)\n",
    "\n",
    "train_data_stns = (x_train_stns, y_train_stns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models with tuned HP\n",
    "output_size = 1\n",
    "model_lta = stocksLSTM(x_train_lta.size()[1], hs_lta, nl_lta, output_size, dp_lta)\n",
    "model_sta = stocksLSTM(x_train_sta.size()[1], hs_sta, nl_sta, output_size, dp_sta)\n",
    "model_sts = stocksLSTM(x_train_sts.size()[1], hs_sts, nl_sts, output_size, dp_sts)\n",
    "model_stns = stocksLSTM(x_train_stns.size()[1], hs_stns, nl_stns, output_size, dp_stns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Training Loss: 53.12024164199829\n",
      "Epoch 100:\n",
      "Training Loss: 6.150647629285231\n",
      "Epoch 0:\n",
      "Training Loss: 52.54952275753021\n",
      "Epoch 100:\n",
      "Training Loss: 33.36127424240112\n",
      "Epoch 0:\n",
      "Training Loss: 52.30368655920029\n",
      "Epoch 100:\n",
      "Training Loss: 26.348262026906013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([52.30368655920029,\n",
       "  51.54582995176315,\n",
       "  51.426953077316284,\n",
       "  50.947267174720764,\n",
       "  48.65872746706009,\n",
       "  47.536966383457184,\n",
       "  46.97531741857529,\n",
       "  46.13974440097809,\n",
       "  46.180207282304764,\n",
       "  46.11367058753967,\n",
       "  46.13580694794655,\n",
       "  46.251478016376495,\n",
       "  46.29696208238602,\n",
       "  46.86714246869087,\n",
       "  47.75264385342598,\n",
       "  45.94577306509018,\n",
       "  47.75531557202339,\n",
       "  46.25571918487549,\n",
       "  46.54837319254875,\n",
       "  46.8564487695694,\n",
       "  46.88105607032776,\n",
       "  46.38868635892868,\n",
       "  47.79576623439789,\n",
       "  45.79392620921135,\n",
       "  47.56597054004669,\n",
       "  46.122165113687515,\n",
       "  45.881316512823105,\n",
       "  49.885848343372345,\n",
       "  45.4810888171196,\n",
       "  46.085785895586014,\n",
       "  45.67219805717468,\n",
       "  45.58818903565407,\n",
       "  45.09038320183754,\n",
       "  44.43429210782051,\n",
       "  43.64019954204559,\n",
       "  41.854240626096725,\n",
       "  42.26430416107178,\n",
       "  42.70684140920639,\n",
       "  41.919868499040604,\n",
       "  41.09028574824333,\n",
       "  38.81782707571983,\n",
       "  38.772187143564224,\n",
       "  38.738134652376175,\n",
       "  38.0724393427372,\n",
       "  38.42644736170769,\n",
       "  37.384124130010605,\n",
       "  37.98183351755142,\n",
       "  38.993421852588654,\n",
       "  36.96579173207283,\n",
       "  38.17649510502815,\n",
       "  41.26167070865631,\n",
       "  37.839936167001724,\n",
       "  36.73430275917053,\n",
       "  37.33758273720741,\n",
       "  35.44009381532669,\n",
       "  37.86926540732384,\n",
       "  36.54666182398796,\n",
       "  39.52181234955788,\n",
       "  38.57547977566719,\n",
       "  38.308161437511444,\n",
       "  34.32749702036381,\n",
       "  34.22030486166477,\n",
       "  35.035351276397705,\n",
       "  33.790935322642326,\n",
       "  35.348777398467064,\n",
       "  30.964028522372246,\n",
       "  31.716273978352547,\n",
       "  36.04180085659027,\n",
       "  36.35408726334572,\n",
       "  35.1931079775095,\n",
       "  33.9580979347229,\n",
       "  31.719518676400185,\n",
       "  30.064047425985336,\n",
       "  31.073668628931046,\n",
       "  36.02332980930805,\n",
       "  33.03773573040962,\n",
       "  32.768144726753235,\n",
       "  34.206440925598145,\n",
       "  34.32753840088844,\n",
       "  32.98768803477287,\n",
       "  32.90560945868492,\n",
       "  31.668674677610397,\n",
       "  28.300505980849266,\n",
       "  33.74882635474205,\n",
       "  30.278827145695686,\n",
       "  29.92642968893051,\n",
       "  32.9025422334671,\n",
       "  33.4127621948719,\n",
       "  29.27969640493393,\n",
       "  29.475950464606285,\n",
       "  30.77451780438423,\n",
       "  29.995944797992706,\n",
       "  27.88300535082817,\n",
       "  35.829762905836105,\n",
       "  28.817207768559456,\n",
       "  28.957153484225273,\n",
       "  31.500216484069824,\n",
       "  31.090732514858246,\n",
       "  32.02618381381035,\n",
       "  28.35028898715973,\n",
       "  26.348262026906013,\n",
       "  26.859502717852592,\n",
       "  26.114338248968124,\n",
       "  26.273021385073662,\n",
       "  30.357237458229065,\n",
       "  38.53696793317795,\n",
       "  29.233685165643692,\n",
       "  27.904366940259933,\n",
       "  28.00102648139,\n",
       "  27.213184878230095,\n",
       "  26.436861857771873,\n",
       "  27.033822625875473,\n",
       "  30.428312987089157,\n",
       "  29.04926311969757,\n",
       "  25.857197508215904,\n",
       "  24.15468393266201,\n",
       "  29.376111537218094,\n",
       "  24.095934703946114,\n",
       "  24.054223529994488,\n",
       "  28.321503922343254,\n",
       "  34.84013640880585,\n",
       "  32.588717207312584,\n",
       "  28.498177707195282,\n",
       "  25.24591988325119,\n",
       "  31.98202282190323,\n",
       "  27.955796360969543,\n",
       "  23.231799498200417,\n",
       "  23.34274762123823,\n",
       "  26.796486720442772,\n",
       "  30.01137214899063,\n",
       "  26.14139335602522,\n",
       "  22.254477255046368,\n",
       "  23.86565301567316,\n",
       "  30.226798251271248,\n",
       "  25.33349473774433,\n",
       "  24.520635336637497,\n",
       "  21.657570868730545,\n",
       "  24.050754263997078,\n",
       "  24.339919701218605,\n",
       "  24.93991257995367,\n",
       "  26.187184423208237,\n",
       "  25.468243584036827,\n",
       "  23.819109305739403,\n",
       "  27.923079192638397,\n",
       "  24.97625745087862,\n",
       "  22.199780829250813,\n",
       "  23.55278754234314,\n",
       "  24.857588320970535,\n",
       "  25.25933074951172,\n",
       "  25.76489444077015,\n",
       "  20.79339325428009,\n",
       "  23.486812226474285,\n",
       "  22.871918089687824,\n",
       "  22.963025897741318,\n",
       "  25.037441223859787,\n",
       "  19.96932962536812,\n",
       "  21.74479280412197,\n",
       "  24.41266069561243,\n",
       "  21.436456210911274,\n",
       "  21.10388407856226,\n",
       "  21.419344410300255,\n",
       "  20.52172127366066,\n",
       "  26.215324483811855,\n",
       "  21.96927536278963,\n",
       "  18.99787799268961,\n",
       "  20.04906179010868,\n",
       "  20.439183849841356,\n",
       "  21.108305245637894,\n",
       "  26.857197538018227,\n",
       "  25.60640048980713,\n",
       "  22.77782154083252,\n",
       "  22.803821563720703,\n",
       "  23.62235526740551,\n",
       "  27.424373492598534,\n",
       "  29.728696182370186,\n",
       "  21.616476960480213,\n",
       "  18.70666414499283,\n",
       "  21.999167025089264,\n",
       "  25.296151138842106,\n",
       "  20.45424970239401,\n",
       "  19.950169689953327,\n",
       "  21.88599030300975,\n",
       "  27.902130231261253,\n",
       "  24.326581984758377,\n",
       "  24.833393573760986,\n",
       "  23.768337309360504,\n",
       "  19.78973589837551,\n",
       "  20.381028227508068,\n",
       "  19.710484825074673,\n",
       "  18.346331920474768,\n",
       "  18.002348598092794,\n",
       "  16.594055760651827,\n",
       "  15.746772618964314,\n",
       "  15.802920395508409,\n",
       "  16.500521786510944,\n",
       "  21.698005326092243,\n",
       "  19.804773861542344,\n",
       "  17.34800434857607,\n",
       "  16.270129341632128,\n",
       "  19.602605905383825],\n",
       " [])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train models with tuned HP\n",
    "EPOCHS = 100*2\n",
    "#train(model_lta, EPOCHS, train_data_lta, valid_data=None, lr=lr_lta)\n",
    "train(model_sta, EPOCHS, train_data_sta, valid_data=None, lr=lr_sta*10)\n",
    "train(model_sts, EPOCHS, train_data_sts, valid_data=None, lr=lr_sts*10)\n",
    "train(model_stns, EPOCHS, train_data_stns, valid_data=None, lr=lr_stns*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: short-term data and all parameters \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      1.00      1.00        35\n",
      "         1.0       1.00      1.00      1.00        29\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "TESTING: short-term data and all parameters \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.71      0.91      0.80        11\n",
      "         1.0       0.67      0.33      0.44         6\n",
      "\n",
      "    accuracy                           0.71        17\n",
      "   macro avg       0.69      0.62      0.62        17\n",
      "weighted avg       0.70      0.71      0.67        17\n",
      "\n",
      "TRAINING: short-term data and selected parameters, including StockTwits sentiment \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.79      0.74      0.76        35\n",
      "         1.0       0.71      0.76      0.73        29\n",
      "\n",
      "    accuracy                           0.75        64\n",
      "   macro avg       0.75      0.75      0.75        64\n",
      "weighted avg       0.75      0.75      0.75        64\n",
      "\n",
      "TESTING: short-term data and selected parameters, including StockTwits sentiment \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.69      0.82      0.75        11\n",
      "         1.0       0.50      0.33      0.40         6\n",
      "\n",
      "    accuracy                           0.65        17\n",
      "   macro avg       0.60      0.58      0.58        17\n",
      "weighted avg       0.62      0.65      0.63        17\n",
      "\n",
      "TRAINING: short-term data and selected parameters, excluding StockTwits sentiment \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.70      0.74      0.72        35\n",
      "         1.0       0.67      0.62      0.64        29\n",
      "\n",
      "    accuracy                           0.69        64\n",
      "   macro avg       0.68      0.68      0.68        64\n",
      "weighted avg       0.69      0.69      0.69        64\n",
      "\n",
      "TESTING: short-term data and selected parameters, excluding StockTwits sentiment \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.65      1.00      0.79        11\n",
      "         1.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.65        17\n",
      "   macro avg       0.32      0.50      0.39        17\n",
      "weighted avg       0.42      0.65      0.51        17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandrosaenz/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alejandrosaenz/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alejandrosaenz/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and compute testing indicators\n",
    "    # long-term data and all parameters\n",
    "# hs = None\n",
    "# train_preds_lta, hs = model_lta(x_train_lta.unsqueeze(0), hs)\n",
    "# test_preds_lta, hs = model_lta(x_test_lta.unsqueeze(0), hs)\n",
    "# test_preds_class_lta = np.where(test_preds_lta > 0, 1, -1)\n",
    "# print('long-term data and all parameters \\n', classification_report(y_test_lta, test_preds_class_lta))\n",
    "    # short-term data and all parameters\n",
    "hs = None\n",
    "train_preds_sta, hs = model_sta(x_train_sta.unsqueeze(0), hs)\n",
    "train_preds_class_sta = np.where(train_preds_sta > 0, 1, -1)\n",
    "print('TRAINING: short-term data and all parameters \\n', classification_report(y_train_sta, train_preds_class_sta))\n",
    "\n",
    "test_preds_sta, hs = model_sta(x_test_sta.unsqueeze(0), hs)\n",
    "test_preds_class_sta = np.where(test_preds_sta > 0, 1, -1)\n",
    "print('TESTING: short-term data and all parameters \\n', classification_report(y_test_sta, test_preds_class_sta))\n",
    "\n",
    "   # short-term data and selected parameters, including StockTwits sentiment \n",
    "hs = None\n",
    "train_preds_sts, hs = model_sts(x_train_sts.unsqueeze(0), hs)\n",
    "train_preds_class_sts = np.where(train_preds_sts > 0, 1, -1)\n",
    "print('TRAINING: short-term data and selected parameters, including StockTwits sentiment \\n', classification_report(y_train_sts, train_preds_class_sts))\n",
    "\n",
    "test_preds_sts, hs = model_sts(x_test_sts.unsqueeze(0), hs)\n",
    "test_preds_class_sts = np.where(test_preds_sts > 0, 1, -1)\n",
    "print('TESTING: short-term data and selected parameters, including StockTwits sentiment \\n', classification_report(y_test_sts, test_preds_class_sts))\n",
    "\n",
    "   # short-term data and selected parameters, excluding StockTwits sentiment \n",
    "hs = None\n",
    "train_preds_stns, hs = model_stns(x_train_stns.unsqueeze(0), hs)\n",
    "train_preds_class_stns = np.where(train_preds_stns > 0, 1, -1)\n",
    "print('TRAINING: short-term data and selected parameters, excluding StockTwits sentiment \\n', classification_report(y_train_stns, train_preds_class_stns))\n",
    "\n",
    "test_preds_stns, hs = model_stns(x_test_stns.unsqueeze(0), hs)\n",
    "test_preds_class_stns = np.where(test_preds_stns > 0, 1, -1)\n",
    "print('TESTING: short-term data and selected parameters, excluding StockTwits sentiment \\n', classification_report(y_test_stns, test_preds_class_stns))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "659f69cde96c241df085b9377ee039d2f5fd3321c931e7e679bf4e3bffd366fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
